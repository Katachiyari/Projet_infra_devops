# ğŸš€ Projet Infra DevSecOps â€“ Proxmox Â· Terraform Â· Ansible

![status](https://img.shields.io/badge/status-en%20cours-brightgreen)
![stack](https://img.shields.io/badge/stack-DevSecOps-blueviolet)
![infra](https://img.shields.io/badge/infra-Proxmox%209.1.1-orange)
![automation](https://img.shields.io/badge/automation-100%25%20IaC-success)

> Esprit startup, infra codÃ©e, sÃ©curisÃ©e et reproductible. Toute la plateforme est pensÃ©e **HTTP backend / HTTPS frontend** avec une **PKI locale** et un **reverse-proxy Nginx** comme porte dâ€™entrÃ©e unique.

---

## ğŸ§© Vision globale

Ce dÃ©pÃ´t dÃ©crit une stack DevSecOps complÃ¨te sur Proxmox :

- **Terraform** : crÃ©e les VMs, configure rÃ©seau & SSH (utilisateur `ansible` + clÃ© publique).
- **cloud-init** : bootstrap unique des OS (qemu-guest-agent, durcissement SSH, sudoers).
- **Ansible** : dÃ©ploie les services applicatifs de faÃ§on **idempotente** (DNS, PKI, reverse-proxy, Harbor/Portainer, monitoring, â€¦).
- **Docker / Docker Compose** : exÃ©cution des services.
- **Trivy** : scan systÃ©matique des images (CRITICAL/HIGH = âŒ).

Le tout est structurÃ© autour dâ€™un principe fort : **SSOT (Single Source of Truth)**. Une seule source pour chaque vÃ©ritÃ© (clÃ© SSH, IPs, DNS, certificatsâ€¦), tout le reste est dÃ©rivÃ© automatiquement.

---

## ğŸ—ï¸ Architecture actuelle (missions rÃ©alisÃ©es)

Architecture rÃ©seau : `172.16.100.0/24` â€“ domaine : `lab.local`.

### âœ… Mission 0 â€“ PKI CA locale

- RÃ´le : `Ansible/roles/pki_ca/`.
- GÃ©nÃ©ration de la **Lab Root CA** (`root-ca.crt/key`) et du certificat wildcard `*.lab.local`.
- Installation de la CA dans le trust systÃ¨me des VMs.
- Tous les certificats serveurs (Nginx) sont Ã©mis par cette CA.

ğŸ‘‰ DÃ©tails : PKI et flux TLS documentÃ©s dans [Docs/stackGlobal/SSOT-DevSecOps-stack.md](Docs/stackGlobal/SSOT-DevSecOps-stack.md).

### âœ… Mission 1 â€“ Reverse-proxy Nginx (172.16.100.253)

- RÃ´le : `Ansible/roles/nginx_reverse_proxy/`.
- Nginx en conteneur (`nginx:1.25-alpine`) avec :
	- Terminaison TLS pour `*.lab.local` (certificat wildcard).
	- Redirection **HTTP â†’ HTTPS**.
	- En-tÃªtes de sÃ©curitÃ© (HSTS, X-Frame-Options, X-Content-Type-Options, X-XSS-Protection, Referrer-Policyâ€¦).
	- Rate-limiting et logs JSON.
- Upstreams HTTP vers les backends : Harbor, Portainer, (futur GitLab, Taiga, EdgeDocâ€¦).

### âœ… Mission 2 â€“ Harbor + Portainer (172.16.100.50)

- RÃ´les : `Ansible/roles/harbor/` et `Ansible/roles/portainer/`.
- **Harbor** (registre dâ€™images, HTTP interne) :
	- ExposÃ© en HTTP sur `172.16.100.50:80`.
	- `external_url` = `https://harbor.lab.local` (via Nginx).
- **Portainer CE** (UI Docker, HTTP interne) :
	- ExposÃ© en HTTP sur `172.16.100.50:9000`.
- **SÃ©curitÃ©** :
	- UFW sur la VM autorise 80/9000 **uniquement** depuis `172.16.100.253` (reverse-proxy).
	- Trivy intÃ©grÃ© dans les rÃ´les pour scanner les images clÃ©s.

### âœ… Stack monitoring (172.16.100.60)

- RÃ´le : `Ansible/roles/monitoring/`.
- VM `monitoring-stack` avec :
	- **Prometheus** : `http://prometheus.lab.local:9090/`.
	- **Grafana** : `http://grafana.lab.local:3000/`.
	- **Alertmanager** : `http://alertmanager.lab.local:9093/`.
- **Node Exporter** dÃ©ployÃ© sur les VMs pour exposer les mÃ©triques systÃ¨me.

---

## ğŸ” Flux end-to-end (HTTP backend / HTTPS frontend)

Exemple : `https://harbor.lab.local/` â†’ Harbor.

1. **DNS Bind9** renvoie `harbor.lab.local` â†’ `172.16.100.253` (reverse-proxy).
2. Le navigateur se connecte en **HTTPS** Ã  Nginx qui prÃ©sente le wildcard `*.lab.local` (signÃ© par la Lab Root CA).
3. Nginx proxifie en **HTTP** vers `172.16.100.50:80` (Harbor backend).
4. La rÃ©ponse revient chiffrÃ©e vers le client.

MÃªme logique pour `https://portainer.lab.local/` â†’ `172.16.100.50:9000`.

ğŸ‘‰ La doc dÃ©taillÃ©e (DNS, flux, troubleshooting) est dans [Docs/stackGlobal/SSOT-DevSecOps-stack.md](Docs/stackGlobal/SSOT-DevSecOps-stack.md).

---

## âš™ï¸ Pipeline IaC de bout en bout

### ğŸ” Connexion 100% automatisÃ©e

1. **SSOT clÃ© SSH** : `keys/â€¦ed25519.pub` rÃ©fÃ©rencÃ©e dans `terraform.tfvars` (`ssh_public_key`).
2. **Terraform** crÃ©e les VMs Proxmox et pousse la clÃ© via `initialization.user_account`.
3. **cloud-init** fait le bootstrap (packages, sshd, sudoers) sans recrÃ©er lâ€™utilisateur.
4. **Terraform** gÃ©nÃ¨re lâ€™inventaire Ansible : `Ansible/inventory/terraform.generated.yml`.

### ğŸ§¾ Fichiers sensibles

- Secrets non versionnÃ©s : `terraform.tfvars`, autres `*.tfvars`.
- State non versionnÃ© : `*.tfstate*` (idÃ©alement backend distant).

### ğŸš€ DÃ©marrage (happy path)

1. Copier `terraform.tfvars.example` â†’ `terraform.tfvars` et adapter.
2. `terraform init`
3. `terraform plan -input=false`
4. `terraform apply -input=false`
5. Dans `Ansible/` :
	 - `./bootstrap.sh`
	 - `./run-ping-test.sh` (ou `--bastion` selon ton contexte)
	 - Playbooks applicatifs (PKI, reverse-proxy, Harbor/Portainer, monitoringâ€¦).

Astuce : `terraform plan -var-file=terraform.tfvars -input=false` pour forcer le var-file.

---

## ğŸ“š Documentation SSOT

- Vue dâ€™ensemble DevSecOps (PKI, Nginx, Harbor/Portainer, monitoring, DNS, flux) :
	- [Docs/stackGlobal/SSOT-DevSecOps-stack.md](Docs/stackGlobal/SSOT-DevSecOps-stack.md)
- DNS / Bind9 :
	- [Docs/bind9/bind9.md](Docs/bind9/bind9.md)
- Monitoring stack :
	- [Docs/stackMonitoring/stackMonitoring.md](Docs/stackMonitoring/stackMonitoring.md)
- GitLab (design & contraintes, en amont de la Mission 3) :
	- [Docs/gitLab/gitLab.md](Docs/gitLab/gitLab.md)

---

## ğŸŒ URLs principales (stack dÃ©jÃ  livrÃ©e)

Une fois la stack dÃ©ployÃ©e **et la CA importÃ©e dans le navigateur** :

- ğŸ” Harbor : `https://harbor.lab.local/`
- ğŸ§­ Portainer : `https://portainer.lab.local/`
- ğŸ“ˆ Prometheus : `http://prometheus.lab.local:9090/`
- ğŸ“Š Grafana : `http://grafana.lab.local:3000/`
- ğŸš¨ Alertmanager : `http://alertmanager.lab.local:9093/`

---

## ğŸ”® Roadmap â€“ Missions Ã  venir (ia.txt)

> Les prochaines Ã©tapes sont dÃ©jÃ  spÃ©cifiÃ©es dans [ia.txt](ia.txt). La stack actuelle a Ã©tÃ© pensÃ©e pour les accueillir **sans refonte**.

### ğŸš§ Mission 3 â€“ GitLab (Ã  venir)

- VM dÃ©diÃ©e (GitLab) avec services HTTP :
	- GitLab web : `172.16.100.40:80`.
	- Registry : `172.16.100.40:5050`.
- Reverse-proxy Nginx en frontal :
	- `https://gitlab.lab.local/` â†’ backend HTTP GitLab.
	- `https://registry.gitlab.lab.local/` â†’ backend HTTP registry.
- IntÃ©gration GitLab Runner, CI/CD, registry Docker.

### ğŸš§ Mission 4 â€“ Taiga + EdgeDoc (Ã  venir)

- VM applicative partagÃ©e (ou dÃ©diÃ©e selon design final) :
	- Taiga (gestion de projet agile) en HTTP (port 80).
	- EdgeDoc (docs collaboratives) en HTTP (port 8080).
- Exposition via reverse-proxy :
	- `https://taiga.lab.local/`.
	- `https://edgedoc.lab.local/`.

Les mÃªmes patterns sâ€™appliquent : **HTTP interne, HTTPS externe, PKI locale, Trivy, UFW restrictif, Ansible idempotent**.

---

## ğŸ¤ Contribuer / Faire Ã©voluer la stack

- Ajouter une nouvelle appli = ajouter **une mission** :
	- RÃ´le Ansible dÃ©diÃ© (`roles/<app>/`).
	- Backends HTTP sÃ©curisÃ©s via UFW.
	- EntrÃ©e Nginx dans le reverse-proxy.
	- EntrÃ©es DNS Bind9 cohÃ©rentes.
- Garder la logique SSOT :
	- Terraform pour lâ€™infra & inventaire.
	- Ansible pour la configuration.
	- Docs sous `Docs/` comme vÃ©ritÃ© fonctionnelle.

Cette base est prÃªte pour des features plus Â« startup Â» : CI/CD GitLab, intÃ©gration Harbor, scans Trivy en pipeline, dashboards Grafana pour lâ€™observabilitÃ©, etc. Letâ€™s build on top ğŸš€
